{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregular_transformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderClassifierRegular\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mearly_stopper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep_set_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepSetAttentionModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUDModel\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mip_nets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationPredictionModel\n",
      "File \u001b[0;32m~/EHR_sensor_representations/scripts/models/deep_set_attention.py:10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseft_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     build_dense_dropout_model,\n\u001b[1;32m     12\u001b[0m     PaddedToSegments,\n\u001b[1;32m     13\u001b[0m     Segmentpooling,\n\u001b[1;32m     14\u001b[0m     cumulative_softmax_weighting,\n\u001b[1;32m     15\u001b[0m     segment_softmax,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m dense_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel_initializer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe_uniform\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPositionalEncodingTF\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from scripts.mortality_part_preprocessing import load_pad_separate\n",
    "from scripts.mortality_part_preprocessing import MortalityDataset, PairedDataset\n",
    "from scripts.models.regular_transformer import EncoderClassifierRegular\n",
    "from scripts.models.early_stopper import EarlyStopping\n",
    "from scripts.models.deep_set_attention import DeepSetAttentionModel\n",
    "from scripts.models.grud import GRUDModel\n",
    "from scripts.models.ip_nets import InterpolationPredictionModel\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "37\n",
      "[83.11079574756315, 119.44102735156771, 359.6545383880742, 513.2573910788382, 2.894396249926148]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "file = \"normalization_physionet2012_1.json\"\n",
    "path = \"/zhome/be/1/138857/EHR_sensor_representations/scripts/P12data/\"\n",
    "import json\n",
    "with open(path + file, \"r\") as f:\n",
    "    norm_data = json.load(f)\n",
    "# Print time-series feature names\n",
    "print(type(norm_data[\"ts_means\"]))  # Should print <class 'list'>\n",
    "print(len(norm_data[\"ts_means\"]))   # How many elements?\n",
    "print(norm_data[\"ts_means\"][:5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed datasets from ./processed_datasets\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_pos.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_neg.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_val.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_test.h5\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"physionet2012\"  # Change this if needed\n",
    "base_path = \"/zhome/be/1/138857/EHR_sensor_representations/scripts/P12data/split_1\"  # Path to extracted files\n",
    "split_index = 1  # Choose the dataset split\n",
    "save_path = \"./processed_datasets\"  # Where the preprocessed data will be stored\n",
    "\n",
    "# Load the dataset\n",
    "mortality_pair, mortality_val, mortality_test = load_pad_separate(\n",
    "    dataset_id, base_path, split_index, save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_pair = mortality_pair\n",
    "train_batch_size = batch_size // 2 \n",
    "\n",
    "train_collate_fn = PairedDataset.paired_collate_fn_truncate\n",
    "train_dataloader = DataLoader(train_pair, train_batch_size, shuffle=True, num_workers=16, collate_fn=train_collate_fn, pin_memory=True)\n",
    "train_dataloader = DataLoader(train_pair, train_batch_size, shuffle=True, num_workers=16, collate_fn=train_collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fec8c8326d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0 \n",
    "for batch in tqdm.tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "    c += 1\n",
    "    if c < 2:\n",
    "        data, times, static, labels, mask, delta = batch\n",
    "        print(data)\n",
    "    else:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_features(num_features, batch_size, time_steps, device):\n",
    "    \"\"\"\n",
    "    Creates a one-hot encoding matrix for all features and generates corresponding mask and delta.\n",
    "\n",
    "    Args:\n",
    "    - num_features (int): Number of original features in X.\n",
    "    - batch_size (int): Number of samples in the batch.\n",
    "    - time_steps (int): Number of time steps (sequence length).\n",
    "    - device (str): CUDA or CPU device.\n",
    "\n",
    "    Returns:\n",
    "    - feature_one_hot (torch.Tensor): One-hot encoded feature matrix.\n",
    "    - one_hot_mask (torch.Tensor): Corresponding mask for one-hot features.\n",
    "    - one_hot_delta (torch.Tensor): Corresponding delta values (zero since one-hot is static).\n",
    "    \"\"\"\n",
    "\n",
    "    # Identity matrix for one-hot encoding (F, F)\n",
    "    feature_one_hot = torch.eye(num_features, device=device)  # Shape: (F, F)\n",
    "\n",
    "    # Expand to match batch and time dimensions\n",
    "    feature_one_hot = feature_one_hot.unsqueeze(0).unsqueeze(2)  # Shape: (1, F, 1, F)\n",
    "    feature_one_hot = feature_one_hot.expand(batch_size, -1, time_steps, -1)  # Shape: (B, F, T, F)\n",
    "\n",
    "    # Reshape for concatenation with data (flatten the one-hot encoding)\n",
    "    feature_one_hot = feature_one_hot.reshape(batch_size, num_features * num_features, time_steps)  # (B, F*F, T)\n",
    "\n",
    "    # Create a corresponding mask (1s for all one-hot encoded features)\n",
    "    one_hot_mask = torch.ones_like(feature_one_hot, dtype=torch.float32, device=device)  # (B, F*F, T)\n",
    "\n",
    "    # Create a corresponding delta (0s since one-hot encoding does not change over time)\n",
    "    one_hot_delta = torch.zeros_like(feature_one_hot, dtype=torch.float32, device=device)  # (B, F*F, T)\n",
    "\n",
    "    return feature_one_hot, one_hot_mask, one_hot_delta\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    output_path,\n",
    "    epochs,\n",
    "    patience,\n",
    "    device,\n",
    "    model_type,\n",
    "    lr,\n",
    "    early_stop_criteria,\n",
    "    model_args,\n",
    "    expand_features=True,  # NEW: Toggle one-hot encoding\n",
    "    **kwargs,  \n",
    "):\n",
    "    \"\"\"\n",
    "    Training function with an option to expand feature space using one-hot encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    # assign GPU\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    device = torch.device(dev)\n",
    "    \n",
    "    iterable_inner_dataloader = iter(train_dataloader)\n",
    "    test_batch = next(iterable_inner_dataloader)\n",
    "    max_seq_length = test_batch[0].shape[2]  # Time steps (T)\n",
    "    sensor_count = test_batch[0].shape[1]  # Features (F)\n",
    "    static_size = test_batch[2].shape[1]  # Static feature size\n",
    "\n",
    "    # Define the model\n",
    "    if model_type == \"grud\":\n",
    "        model = GRUDModel(\n",
    "            input_dim=sensor_count + (sensor_count * sensor_count if expand_features else 0),  # Adjust for expanded input\n",
    "            static_dim=static_size,\n",
    "            output_dims=2,\n",
    "            device=device,\n",
    "            recurrent_n_units=128,  #ADDED\n",
    "            recurrent_dropout=0.3, #ADDED \n",
    "            dropout=0.0, #ADDED \n",
    "            **model_args\n",
    "        )\n",
    "    elif model_type == \"ipnets\":\n",
    "        model = InterpolationPredictionModel(\n",
    "            output_dims=2,\n",
    "            sensor_count=sensor_count + (sensor_count if expand_features else 0),  # Adjust for expanded input\n",
    "            **model_args\n",
    "        )\n",
    "    elif model_type == \"seft\":\n",
    "        model = DeepSetAttentionModel(\n",
    "            output_activation=None,\n",
    "            n_modalities=sensor_count + (sensor_count if expand_features else 0),\n",
    "            output_dims=2,\n",
    "            **model_args\n",
    "        )\n",
    "    elif model_type == \"transformer\":\n",
    "        model = EncoderClassifierRegular(\n",
    "            num_classes=2,\n",
    "            device=device,\n",
    "            max_timepoint_count=max_seq_length,\n",
    "            sensors_count=sensor_count + (sensor_count if expand_features else 0),\n",
    "            static_count=static_size,\n",
    "            return_intermediates=False,\n",
    "            **model_args\n",
    "        )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, verbose=True, path=f\"{output_path}/checkpoint.pt\"\n",
    "    )  \n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train().to(device)\n",
    "        loss_list = []\n",
    "\n",
    "        c = 0 \n",
    "        for batch in tqdm.tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "\n",
    "            data, times, static, labels, mask, delta = batch\n",
    "\n",
    "            # Create one-hot encoding for features and concatenate\n",
    "            batch_size = data.shape[0]\n",
    "           # Get expanded one-hot encoded features, mask, and delta\n",
    "            one_hot_features, one_hot_mask, one_hot_delta = get_one_hot_features(sensor_count, batch_size, data.shape[2], device)\n",
    "\n",
    "            # Concatenate one-hot encoding to data, mask, and delta\n",
    "            data = torch.cat([data, one_hot_features], dim=1)\n",
    "            mask = torch.cat([mask, one_hot_mask], dim=1)\n",
    "            delta = torch.cat([delta, one_hot_delta], dim=1)\n",
    "\n",
    "            if model_type != \"grud\":\n",
    "                data = data.to(device)\n",
    "                static = static.to(device)\n",
    "                times = times.to(device)\n",
    "                mask = mask.to(device)\n",
    "                delta = delta.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(\n",
    "                x=data, static=static, time=times, sensor_mask=mask, delta=delta\n",
    "            )\n",
    "            predictions = predictions.squeeze(-1)\n",
    "            loss = criterion(predictions.cpu(), labels)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        accum_loss = np.mean(loss_list)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval().to(device)\n",
    "        labels_list = torch.LongTensor([])\n",
    "        predictions_list = torch.FloatTensor([])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                data, times, static, labels, mask, delta = batch\n",
    "\n",
    "                # Create one-hot encoding for features and concatenate\n",
    "                batch_size = data.shape[0]\n",
    "                one_hot_features, one_hot_mask, one_hot_delta = get_one_hot_features(sensor_count, batch_size, data.shape[2], device)\n",
    "\n",
    "                # Concatenate one-hot encoding to data, mask, and delta\n",
    "                data = torch.cat([data, one_hot_features], dim=1)\n",
    "                mask = torch.cat([mask, one_hot_mask], dim=1)\n",
    "                delta = torch.cat([delta, one_hot_delta], dim=1)\n",
    "\n",
    "                if model_type != \"grud\":\n",
    "                    data = data.to(device)\n",
    "                    static = static.to(device)\n",
    "                    times = times.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    delta = delta.to(device)\n",
    "\n",
    "                predictions = model(\n",
    "                    x=data, static=static, time=times, sensor_mask=mask, delta=delta\n",
    "                )\n",
    "                predictions = predictions.squeeze(-1)\n",
    "                predictions_list = torch.cat((predictions_list, predictions.cpu()), dim=0)\n",
    "        \n",
    "        val_loss = criterion(predictions_list.cpu(), labels_list)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {accum_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "        if early_stop_criteria == \"loss\":\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return val_loss, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed datasets from ./processed_datasets\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_pos.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_neg.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_val.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_test.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▏                                                                                                                             | 4/126 [00:11<05:44,  2.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_pair, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mPairedDataset\u001b[38;5;241m.\u001b[39mpaired_collate_fn_truncate, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mMortalityDataset\u001b[38;5;241m.\u001b[39mnon_pair_collate_fn_truncate, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m train_loss, trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrud\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable one-hot encoding\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 139\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, val_dataloader, output_path, epochs, patience, device, model_type, lr, early_stop_criteria, model_args, expand_features, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     delta \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    137\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 139\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    143\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions\u001b[38;5;241m.\u001b[39mcpu(), labels)\n",
      "File \u001b[0;32m~/EHR_sensor_representations/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EHR_sensor_representations/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/EHR_sensor_representations/scripts/models/grud.py:438\u001b[0m, in \u001b[0;36mGRUDModel.forward\u001b[0;34m(self, x, static, time, sensor_mask, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m time_mask \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mcount_nonzero(time, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    436\u001b[0m time_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m#0th timepoint is always valid\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m grud_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_keep_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_prev_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# Get the index of the last valid hidden state for each sequence\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     last_valid_indices \u001b[38;5;241m=\u001b[39m time_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/EHR_sensor_representations/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EHR_sensor_representations/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_id = \"physionet2012\"  # Change this if needed\n",
    "base_path = \"/zhome/be/1/138857/EHR_sensor_representations/scripts/P12data/split_1\"  # Path to extracted files\n",
    "split_index = 1  # Choose the dataset split\n",
    "save_path = \"./processed_datasets\"  # Where the preprocessed data will be stored\n",
    "\n",
    "# Load the dataset\n",
    "train_pair, val, test = load_pad_separate(\n",
    "    dataset_id, base_path, split_index, save_path\n",
    ")\n",
    "\n",
    "batch_size = 64 // 2\n",
    "\n",
    "train_loader = DataLoader(train_pair, batch_size, shuffle=True, num_workers=16, collate_fn=PairedDataset.paired_collate_fn_truncate, pin_memory=True)\n",
    "val_loader = DataLoader(val, batch_size, shuffle=True, num_workers=16, collate_fn=MortalityDataset.non_pair_collate_fn_truncate, pin_memory=True)\n",
    "\n",
    "train_loss, trained_model = train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    output_path=\"./results\",\n",
    "    epochs=2,\n",
    "    patience=5,\n",
    "    device=\"cuda\",\n",
    "    model_type=\"grud\",\n",
    "    lr=0.001,\n",
    "    early_stop_criteria=\"loss\",\n",
    "    model_args={},\n",
    "    expand_features=True  # Enable one-hot encoding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "37*37+37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406\n"
     ]
    }
   ],
   "source": [
    "sensor_count = 37\n",
    "expand_features = True\n",
    "test = sensor_count + (sensor_count * sensor_count if expand_features else 0)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
